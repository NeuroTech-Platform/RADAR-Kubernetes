

# radar-upload-source-connector

![Version: 0.1.1](https://img.shields.io/badge/Version-0.1.1-informational?style=flat-square) ![Type: application](https://img.shields.io/badge/Type-application-informational?style=flat-square) ![AppVersion: 0.5.9](https://img.shields.io/badge/AppVersion-0.5.9-informational?style=flat-square)

A Helm chart for RADAR-base upload kafka connector.

**Homepage:** <https://radar-base.org>

## Maintainers

| Name | Email | Url |
| ---- | ------ | --- |
| Keyvan Hedayati | keyvan@thehyve.nl |  |
| Joris Borgdorff | joris@thehyve.nl |  |
| Nivethika Mahasivam | nivethika@thehyve.nl |  |

## Source Code

* <https://github.com/RADAR-base/radar-upload-source-connector>

## Prerequisites
* Kubernetes 1.17+
* Kubectl 1.17+
* Helm 3.1.0+

## Values

| Key | Type | Default | Description |
|-----|------|---------|-------------|
| replicaCount | int | `2` |  |
| image.repository | string | `"radarbase/radar-connect-upload-source"` |  |
| image.tag | string | `"0.5.9"` |  |
| image.pullPolicy | string | `"IfNotPresent"` |  |
| nameOverride | string | `""` |  |
| fullnameOverride | string | `""` |  |
| service.type | string | `"ClusterIP"` |  |
| service.port | int | `80` |  |
| ingress.enabled | bool | `false` |  |
| ingress.annotations | object | `{}` |  |
| ingress.hosts[0].host | string | `"chart-example.local"` |  |
| ingress.hosts[0].paths | list | `[]` |  |
| ingress.tls | list | `[]` |  |
| resources.requests.cpu | string | `"100m"` |  |
| resources.requests.memory | string | `"800Mi"` |  |
| nodeSelector | object | `{}` |  |
| tolerations | list | `[]` |  |
| affinity | object | `{}` |  |
| zookeeper | string | `"cp-zookeeper-headless:2181"` | Zookeeper URL |
| kafka | string | `"PLAINTEXT://cp-kafka-headless:9092"` | Kafka broker URLs |
| kafka_num_brokers | string | `"3"` | Number of brokers in the cluster |
| schema_registry | string | `"http://cp-schema-registry:8081"` | Schema registry URL |
| managementportal_host | string | `"management-portal"` | Host name of the Management Portal application |
| radar_upload_connect_backend | string | `"radar-upload-connect-backend"` | Host name of the upload connect backend application |
| client_id | string | `"radar_upload_connect"` | OAuth2 Client Id of the Upload connector |
| client_secret | string | `"upload_secret"` | OAuth2 Client secret of the Upload connector |
| poll_interval | int | `60000` | How often the connector should poll for new records from upload connect backend in milliseconds. |
| record_converter_classes | string | `"org.radarbase.connect.upload.converter.altoida.AltoidaConverterFactory,org.radarbase.connect.upload.converter.axivity.AxivityConverterFactory,org.radarbase.connect.upload.converter.oxford.WearableCameraConverterFactory,org.radarbase.connect.upload.converter.gaitup.Physilog5ConverterFactory"` | List of converter classes to be activated as comma separated values. |
| uploaderType | string | `"s3"` | Uploader type for converters which directly write the files bypassing the Kafka processing. e.g. images and binaries. |
| s3Endpoint | string | `"http://minio:9000/"` | Target S3 endpoint, if files should be written to a location bypassing the Kafka processing. |
| bucketAccessKey | string | `"access_key"` | Target S3 access key |
| bucketSecretKey | string | `"secret"` | Target S3 secret key |
| targetBucketName | string | `"radar-output-storage"` | Target S3 bucket name |
| task.queueSize | int | `10000` | Maximum number of source records that can be produced at a time, preventing out of memory errors. |
| connect.offsetFlushIntervalMs | int | `5000` | Interval at which to try committing offsets for tasks. See |
| producer | object | `{"batchSize":200000,"bufferMemory":2000000,"compressionType":"lz4"}` | Override kafka producer configs. For more details see https://docs.confluent.io/platform/current/installation/configuration/producer-configs.html |
| producer.compressionType | string | `"lz4"` | The compression type for all data generated by the producer. |
| producer.bufferMemory | int | `2000000` | The total bytes of memory the producer can use to buffer records waiting to be sent to the server. |
| producer.batchSize | int | `200000` | Batch size in bytes to batch records together into fewer requests when multiple records are being sent to the same partition. |